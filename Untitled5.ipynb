{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9IhJ6EHDyMYk+uYAqavp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit-Roby/Project/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lqFd9zHtAXiy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "\n",
        "\"\"\"\n",
        "Implementation based on original paper NeurIPS 2016\n",
        "https://papers.nips.cc/paper/6096-learning-a-probabilistic-latent-space-of-object-shapes-via-3d-generative-adversarial-modeling.pdf\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self, in_channels=1, dim=64, out_conv_channels=512):\n",
        "        super(Discriminator, self).__init__()\n",
        "        conv1_channels = int(out_conv_channels / 8)\n",
        "        conv2_channels = int(out_conv_channels / 4)\n",
        "        conv3_channels = int(out_conv_channels / 2)\n",
        "        self.out_conv_channels = out_conv_channels\n",
        "        self.out_dim = int(dim / 16)\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels=in_channels, out_channels=conv1_channels, kernel_size=4,\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm3d(conv1_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels=conv1_channels, out_channels=conv2_channels, kernel_size=4,\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm3d(conv2_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels=conv2_channels, out_channels=conv3_channels, kernel_size=4,\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm3d(conv3_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv3d(\n",
        "                in_channels=conv3_channels, out_channels=out_conv_channels, kernel_size=4,\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm3d(out_conv_channels),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(out_conv_channels * self.out_dim * self.out_dim * self.out_dim, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        # Flatten and apply linear + sigmoid\n",
        "        x = x.view(-1, self.out_conv_channels * self.out_dim * self.out_dim * self.out_dim)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, in_channels=512, out_dim=64, out_channels=1, noise_dim=200, activation=\"sigmoid\"):\n",
        "        super(Generator, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_dim = out_dim\n",
        "        self.in_dim = int(out_dim / 16)\n",
        "        conv1_out_channels = int(self.in_channels / 2.0)\n",
        "        conv2_out_channels = int(conv1_out_channels / 2)\n",
        "        conv3_out_channels = int(conv2_out_channels / 2)\n",
        "\n",
        "        self.linear = torch.nn.Linear(noise_dim, in_channels * self.in_dim * self.in_dim * self.in_dim)\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(\n",
        "                in_channels=in_channels, out_channels=conv1_out_channels, kernel_size=(4, 4, 4),\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm3d(conv1_out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(\n",
        "                in_channels=conv1_out_channels, out_channels=conv2_out_channels, kernel_size=(4, 4, 4),\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm3d(conv2_out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(\n",
        "                in_channels=conv2_out_channels, out_channels=conv3_out_channels, kernel_size=(4, 4, 4),\n",
        "                stride=2, padding=1, bias=False\n",
        "            ),\n",
        "            nn.BatchNorm3d(conv3_out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.ConvTranspose3d(\n",
        "                in_channels=conv3_out_channels, out_channels=out_channels, kernel_size=(4, 4, 4),\n",
        "                stride=2, padding=1, bias=False\n",
        "            )\n",
        "        )\n",
        "        if activation == \"sigmoid\":\n",
        "            self.out = torch.nn.Sigmoid()\n",
        "        else:\n",
        "            self.out = torch.nn.Tanh()\n",
        "\n",
        "    def project(self, x):\n",
        "        \"\"\"\n",
        "        projects and reshapes latent vector to starting volume\n",
        "        :param x: latent vector\n",
        "        :return: starting volume\n",
        "        \"\"\"\n",
        "        return x.view(-1, self.in_channels, self.in_dim, self.in_dim, self.in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.project(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        return self.out(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_gan3d():\n",
        "    noise_dim = 200\n",
        "    in_channels = 512\n",
        "    dim = 64  # cube volume\n",
        "    model_generator = Generator(in_channels=512, out_dim=dim, out_channels=1, noise_dim=noise_dim)\n",
        "    noise = torch.rand(1, noise_dim)\n",
        "    generated_volume = model_generator(noise)\n",
        "    print(\"Generator output shape\", generated_volume.shape)\n",
        "    model_discriminator = Discriminator(in_channels=1, dim=dim, out_conv_channels=in_channels)\n",
        "    out = model_discriminator(generated_volume)\n",
        "    print(\"Discriminator output\", out)\n",
        "    summary(model_generator, (1, noise_dim))\n",
        "    summary(model_discriminator, (1, 64, 64, 64))\n",
        "\n",
        "\n",
        "test_gan3d()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpcY0vkuAZ2W",
        "outputId": "5ed2c952-7a80-4389-b2a8-135462c83760"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator output shape torch.Size([1, 1, 64, 64, 64])\n",
            "Discriminator output tensor([[0.4733]], grad_fn=<SigmoidBackward0>)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1             [-1, 1, 32768]       6,586,368\n",
            "   ConvTranspose3d-2         [-1, 256, 8, 8, 8]       8,388,608\n",
            "       BatchNorm3d-3         [-1, 256, 8, 8, 8]             512\n",
            "              ReLU-4         [-1, 256, 8, 8, 8]               0\n",
            "   ConvTranspose3d-5      [-1, 128, 16, 16, 16]       2,097,152\n",
            "       BatchNorm3d-6      [-1, 128, 16, 16, 16]             256\n",
            "              ReLU-7      [-1, 128, 16, 16, 16]               0\n",
            "   ConvTranspose3d-8       [-1, 64, 32, 32, 32]         524,288\n",
            "       BatchNorm3d-9       [-1, 64, 32, 32, 32]             128\n",
            "             ReLU-10       [-1, 64, 32, 32, 32]               0\n",
            "  ConvTranspose3d-11        [-1, 1, 64, 64, 64]           4,096\n",
            "          Sigmoid-12        [-1, 1, 64, 64, 64]               0\n",
            "================================================================\n",
            "Total params: 17,601,408\n",
            "Trainable params: 17,601,408\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 67.25\n",
            "Params size (MB): 67.14\n",
            "Estimated Total Size (MB): 134.39\n",
            "----------------------------------------------------------------\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv3d-1       [-1, 64, 32, 32, 32]           4,096\n",
            "       BatchNorm3d-2       [-1, 64, 32, 32, 32]             128\n",
            "         LeakyReLU-3       [-1, 64, 32, 32, 32]               0\n",
            "            Conv3d-4      [-1, 128, 16, 16, 16]         524,288\n",
            "       BatchNorm3d-5      [-1, 128, 16, 16, 16]             256\n",
            "         LeakyReLU-6      [-1, 128, 16, 16, 16]               0\n",
            "            Conv3d-7         [-1, 256, 8, 8, 8]       2,097,152\n",
            "       BatchNorm3d-8         [-1, 256, 8, 8, 8]             512\n",
            "         LeakyReLU-9         [-1, 256, 8, 8, 8]               0\n",
            "           Conv3d-10         [-1, 512, 4, 4, 4]       8,388,608\n",
            "      BatchNorm3d-11         [-1, 512, 4, 4, 4]           1,024\n",
            "        LeakyReLU-12         [-1, 512, 4, 4, 4]               0\n",
            "           Linear-13                    [-1, 1]          32,769\n",
            "          Sigmoid-14                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 11,048,833\n",
            "Trainable params: 11,048,833\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.00\n",
            "Forward/backward pass size (MB): 63.75\n",
            "Params size (MB): 42.15\n",
            "Estimated Total Size (MB): 106.90\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "8hyC-5QIBHnG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Szmitv7JBIiH",
        "outputId": "e1a8e642-47f1-4974-831d-35fcc9697315"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "yoeOgKCCBLIG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trimesh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvnMsZRbBkcZ",
        "outputId": "c7c5a675-5158-4077-b16f-721c95fcbaeb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trimesh\n",
            "  Downloading trimesh-4.3.1-py3-none-any.whl (693 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.8/693.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from trimesh) (1.23.5)\n",
            "Installing collected packages: trimesh\n",
            "Successfully installed trimesh-4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import trimesh\n",
        "\n",
        "class Custom3DDataset(Dataset):\n",
        "    def __init__(self, file_paths, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mesh = trimesh.load_mesh(self.file_paths[idx])\n",
        "\n",
        "        # Preprocess the mesh (e.g., normalize coordinates, convert to voxel grid)\n",
        "        if self.transform:\n",
        "            mesh = self.transform(mesh)\n",
        "\n",
        "        return mesh\n"
      ],
      "metadata": {
        "id": "w5gt4mFJBXMT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a transformation function to preprocess the meshes\n",
        "def preprocess_mesh(mesh):\n",
        "    # Your preprocessing steps here\n",
        "    # Example: normalize coordinates\n",
        "    mesh.apply_translation(-mesh.centroid)\n",
        "    mesh.apply_scale(1 / mesh.scale)\n",
        "\n",
        "    # Convert to voxel grid (example, you might have a different method)\n",
        "    voxel_grid =trimesh.voxel.creation.local_voxelize(mesh,fill=True)\n",
        "\n",
        "    # Convert voxel grid to tensor\n",
        "    voxel_tensor = torch.tensor(voxel_grid.matrix, dtype=torch.float32)\n",
        "\n",
        "    return voxel_tensor\n"
      ],
      "metadata": {
        "id": "lliOfYkmBnw6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vox = preprocess_mesh(file_paths[0])"
      ],
      "metadata": {
        "id": "Kb6zeOMgEga_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install objaverse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9XsFgDGCQBQ",
        "outputId": "6b4ec052-c1d6-4f27-869a-fdaf9e0d6c91"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting objaverse\n",
            "  Downloading objaverse-0.1.7-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from objaverse) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from objaverse) (1.5.3)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from objaverse) (14.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from objaverse) (4.66.2)\n",
            "Collecting loguru (from objaverse)\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2022.11.0 in /usr/local/lib/python3.10/dist-packages (from objaverse) (2023.6.0)\n",
            "Collecting gputil==1.4.0 (from objaverse)\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->objaverse) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->objaverse) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->objaverse) (1.23.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->objaverse) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->objaverse) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->objaverse) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->objaverse) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->objaverse) (1.16.0)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=ef66338454aa612ffaeffee146e71a354ded9381433ce6730f4ebe13cbbd1a8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil, loguru, objaverse\n",
            "Successfully installed gputil-1.4.0 loguru-0.7.2 objaverse-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import objaverse\n",
        "import random\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "NSJTrm6mCg1C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading Uids\n",
        "uids = objaverse.load_uids()\n",
        "len(uids), type(uids)\n",
        "\n",
        "processes = multiprocessing.cpu_count()\n",
        "processes\n",
        "objects = objaverse.load_objects(\n",
        "    uids = random.sample(uids, 100),\n",
        "    download_processes = processes\n",
        ")"
      ],
      "metadata": {
        "id": "gCIrwT6ECUxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the subfolder and parent folder paths\n",
        "parent_dir = '/root/.objaverse/hf-objaverse-v1/glbs'\n",
        "\n",
        "# Walk through the parent directory\n",
        "for subdir, dirs, files in os.walk(parent_dir):\n",
        "    for file in files:\n",
        "        # Construct the file path\n",
        "        file_path = os.path.join(subdir, file)\n",
        "        # Construct the destination path\n",
        "        dest_path = os.path.join(parent_dir, file)\n",
        "        # Move the file to the parent directory\n",
        "        shutil.move(file_path, dest_path)\n",
        "    # If the current directory is not the parent directory, delete the subdirectory\n",
        "    if subdir != parent_dir:\n",
        "        os.rmdir(subdir)"
      ],
      "metadata": {
        "id": "M2YvDQ1PDCVa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_model_paths(base_dir):\n",
        "    model_paths = []\n",
        "    for subdir, dirs, files in os.walk(base_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".glb\"):\n",
        "                file_path = os.path.join(subdir, file)\n",
        "                model_paths.append(file_path)\n",
        "    return model_paths\n",
        "base_dir ='/root/.objaverse/hf-objaverse-v1/glbs'\n",
        "file_paths = get_model_paths(base_dir)"
      ],
      "metadata": {
        "id": "l8uRpj4EDQJF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of file paths to your .glb files\n",
        "file_paths"
      ],
      "metadata": {
        "id": "Njff5HCTDk5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom dataset\n",
        "custom_dataset = Custom3DDataset(file_paths, transform=preprocess_mesh)\n"
      ],
      "metadata": {
        "id": "I6gH5y7UBqur"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define DataLoader\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "-ezlEM9HDrM5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6Gu9sDPDsLd",
        "outputId": "b5279588-ea0f-467a-fd02-0cfc0e178df8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x78f3c3b55d80>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize networks\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Define optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Define loss function (Binary Cross Entropy Loss)\n",
        "adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, _) in enumerate(data_loader):\n",
        "\n",
        "        # ---------------------\n",
        "        #  Train Discriminator\n",
        "        # ---------------------\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Train on real images\n",
        "        real_images = real_images.to(device)\n",
        "        real_labels = torch.full((real_images.size(0),), 1, device=device)\n",
        "        output_real = discriminator(real_images)\n",
        "        loss_real = adversarial_loss(output_real.squeeze(), real_labels)\n",
        "        loss_real.backward()\n",
        "\n",
        "        # Train on fake images\n",
        "        noise = torch.randn(batch_size, noise_dim, device=device)\n",
        "        fake_images = generator(noise)\n",
        "        fake_labels = torch.full((fake_images.size(0),), 0, device=device)\n",
        "        output_fake = discriminator(fake_images.detach())\n",
        "        loss_fake = adversarial_loss(output_fake.squeeze(), fake_labels)\n",
        "        loss_fake.backward()\n",
        "\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # -----------------\n",
        "        #  Train Generator\n",
        "        # -----------------\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # Generate fake images and calculate loss\n",
        "        output = discriminator(fake_images)\n",
        "        loss_G = adversarial_loss(output.squeeze(), real_labels)\n",
        "        loss_G.backward()\n",
        "\n",
        "        optimizer_G.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "9Hr4-f9wA9XO",
        "outputId": "8f1ee330-46a3-409a-fb63-e4a63169b991"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Scene' object has no attribute 'voxelized'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-0681039f6a3a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-841d65b61fd1>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Preprocess the mesh (e.g., normalize coordinates, convert to voxel grid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mmesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmesh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-0e7b06799762>\u001b[0m in \u001b[0;36mpreprocess_mesh\u001b[0;34m(mesh)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Convert to voxel grid (example, you might have a different method)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mvoxel_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoxelized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoxel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Convert voxel grid to tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Scene' object has no attribute 'voxelized'"
          ]
        }
      ]
    }
  ]
}